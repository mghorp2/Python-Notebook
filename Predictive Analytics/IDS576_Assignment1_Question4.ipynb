{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 1 : Generate data from assignment1 code.py. \n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "\n",
    "pickle.dump(X,open('dataX.pickle','wb'))\n",
    "pickle.dump(y,open('dataY.pickle','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports \n",
    "import numpy as np #Represent ndarrays a.k.a. tensors\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "np.random.seed(0) #For repeatability of the experiment\n",
    "import pickle #To read data for this experiment\n",
    "\n",
    "#Read data\n",
    "X = pickle.load(open('dataX.pickle','rb'))\n",
    "y = pickle.load(open('dataY.pickle','rb'))\n",
    "\n",
    "\n",
    "#Define some local varaibles\n",
    "D = X.shape[1] #Number of features\n",
    "K = max(y)+1 #Number of classes assuming class index starts from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 2 : Split the data into test and train (20%:80%)\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X,y, test_size = test_size, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start with an initialize parameters randomly\n",
    "\n",
    "W = 0.01 * np.random.randn(D,K) #random weights\n",
    "#zero bias terms\n",
    "b = np.zeros((1,K))\n",
    "\n",
    "#Initialize hyper-parameter value (not optimized)\n",
    "reg = 1e-3 # regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.53\n"
     ]
    }
   ],
   "source": [
    "# Question 3 : Build a linear classifier assuming the multiclass logistic loss and an `2 regularization for the weights only. \n",
    "#              Report the prediction accuracy on the training data and the test data and show appropriate plots.\n",
    "\n",
    "#Perform batch SGD using backprop\n",
    "\n",
    "#For simplicity we will take the batch size to be the same as number of examples\n",
    "num_examples = trainX.shape[0]\n",
    "\n",
    "#Initial value for the Gradient Descent Parameter\n",
    "step_size = 1e-0 #Also called learning rate\n",
    "\n",
    "num_iterations = 200 #200 iterations\n",
    "\n",
    "# gradient descent loop\n",
    "for i in range(num_iterations): \n",
    "  \n",
    "    # evaluate class scores, [N x K]\n",
    "    # X = 300 obs * 2 features || W = 2 (features) * 3 (output classes) || b = bias weights\n",
    "    scores = np.dot(trainX, W) + b \n",
    "    \n",
    "    # compute the class probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    \n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "    #Compute loss using snippet of LogLoss fucntion created in Q3\n",
    "    #convert the y label to a N * K array \n",
    "    actual = np.zeros(probs.shape)\n",
    "    rows = actual.shape[0]\n",
    "    \n",
    "    actual[np.arange(rows), trainY.astype(int)] = 1\n",
    "    \n",
    "    #compute data loss\n",
    "    data_loss = -1.0 / rows * (np.sum(actual * np.log(probs)))\n",
    "\n",
    "    # L2 regularization term\n",
    "    L2 = np.sum(W ** 2) #not multiplied by 0.5\n",
    "\n",
    "    # the total loss\n",
    "    loss = data_loss + reg * L2\n",
    "\n",
    "    #if i % 10 == 0:\n",
    "        #print \"iteration %d: loss %f\" % (i, loss)\n",
    "        #print \"iteration %d: loss %f\" % (i, loss_old)\n",
    "  \n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),trainY] -= 1\n",
    "    dscores /= num_examples\n",
    "  \n",
    "    # backpropate the gradient to the parameters (W,b)\n",
    "    dW = np.dot(trainX.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "  \n",
    "    dW += reg*W # regularization gradient\n",
    "  \n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db\n",
    "\n",
    "\n",
    "scores = np.dot(testX, W) + b\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "\n",
    "print ('test accuracy: %.2f' % (np.mean(predicted_class == testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for fold 1: 0.47\n",
      "Validation Accuracy for fold 2: 0.50\n",
      "Validation Accuracy for fold 3: 0.58\n",
      "Accuracy: 0.52 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "# Question 4 : Introduce a cross validation scheme and justify your choice of parameters. What is the validation accuracy \n",
    "#              compare to the test accuracy\n",
    "\n",
    "#Using k-fold validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Initial value for the Gradient Descent Parameter -- to be held constant\n",
    "step_size = 1e-0 #Also called learning rate\n",
    "num_iterations = 200 #200 iterations\n",
    "\n",
    "#create k-fold criteria\n",
    "num_folds = 3\n",
    "k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=0)\n",
    "\n",
    "#initialize accuraices (for each fold) to empty list\n",
    "accuracies = list()\n",
    "\n",
    "k = 1 #for printing\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "\n",
    "    # Start with an initialize parameters randomly, for each fold\n",
    "    #random weights\n",
    "    W = 0.01 * np.random.randn(D,K)\n",
    "    #zero bias terms\n",
    "    b = np.zeros((1,K))\n",
    "    \n",
    "    # We use 'list' to copy, in order to 'pop' later on\n",
    "    X_train_k = X[train_indices,]\n",
    "    X_test_k  = X[test_indices,]\n",
    "    \n",
    "    y_train_k = y[train_indices]\n",
    "    y_test_k  = y[test_indices]\n",
    "    \n",
    "    #For simplicity we will take the batch size to be the same as number of examples\n",
    "    num_examples = X_train_k.shape[0]\n",
    "\n",
    "    # gradient descent loop\n",
    "    for i in range(num_iterations): \n",
    "        \n",
    "        # evaluate class scores, [N x K]\n",
    "        # X = 300 obs * 2 features || W = 2 (features) * 3 (output classes) || b = bias weights\n",
    "        scores = np.dot(X_train_k, W) + b \n",
    "\n",
    "        # compute the class probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "        #Compute loss using snippet of LogLoss fucntion created in Q3\n",
    "        #convert the y label to a N * K array \n",
    "        actual = np.zeros(probs.shape)\n",
    "        rows = actual.shape[0]\n",
    "\n",
    "        actual[np.arange(rows), y_train_k.astype(int)] = 1\n",
    "\n",
    "        #compute data loss\n",
    "        data_loss = -1.0 / rows * (np.sum(actual * np.log(probs)))\n",
    "\n",
    "        # L2 regularization term\n",
    "        L2 = np.sum(W ** 2) #not multiplied by 0.5\n",
    "\n",
    "        # the total loss\n",
    "        loss = data_loss + reg * L2\n",
    "\n",
    "        # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),y_train_k] -= 1\n",
    "        dscores /= num_examples\n",
    "\n",
    "        # backpropate the gradient to the parameters (W,b)\n",
    "        dW = np.dot(X_train_k.T, dscores)\n",
    "        db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "        dW += reg*W # regularization gradient\n",
    "\n",
    "        # perform a parameter update\n",
    "        W += -step_size * dW\n",
    "        b += -step_size * db\n",
    "    \n",
    "    #print sum(X_test)\n",
    "    #print y_train_k\n",
    "    \n",
    "    #compute scores for each fold\n",
    "    scores = np.dot(X_test_k, W) + b\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    #print predicted_class\n",
    "    #accuracy for fold\n",
    "    fold_accuracy = np.mean(predicted_class == y_test_k)\n",
    "    \n",
    "    print ('Validation Accuracy for fold %d: %.2f' % (k, fold_accuracy))\n",
    "    accuracies.append(fold_accuracy)\n",
    "    \n",
    "    k += 1\n",
    "\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.mean(accuracies), np.std(accuracies) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 5 : What is the sensitivity of the model’s performance to different learning rates and the number of gradient descent\n",
    "#              iterations. Describe via suitable plots.\n",
    "\n",
    "#Defining expand grid function \n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def expand_grid(data_dict):\n",
    "    rows = itertools.product(*data_dict.values())\n",
    "    return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating training grid\n",
    "trainGrid = expand_grid({ 'num_iters':[10,25,50,75,100,150, 200, 250, 300, 400,500,750,1000],\n",
    "                          'step_size': [1e-3,1e-2,1e-1,5e-1,1,1.5],\n",
    "                         \n",
    "                           #create a column to store accuracies in\n",
    "                          'accuracy_mean': [0.0],\n",
    "                         \n",
    "                           #create a column to store accuract std's in\n",
    "                          'accuracy_std' : [0.0] \n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    num_iters  step_size  accuracy_mean  accuracy_std\n",
      "0          10      0.001       0.310000      0.065320\n",
      "1          10      0.010       0.420000      0.114310\n",
      "2          10      0.100       0.563333      0.067987\n",
      "3          10      0.500       0.520000      0.099331\n",
      "4          10      1.000       0.510000      0.086410\n",
      "5          10      1.500       0.506667      0.082192\n",
      "6          25      0.001       0.290000      0.142361\n",
      "7          25      0.010       0.506667      0.033993\n",
      "8          25      0.100       0.533333      0.123648\n",
      "9          25      0.500       0.506667      0.082192\n",
      "10         25      1.000       0.506667      0.065997\n",
      "11         25      1.500       0.513333      0.067987\n",
      "12         50      0.001       0.463333      0.312552\n",
      "13         50      0.010       0.536667      0.065997\n",
      "14         50      0.100       0.526667      0.118134\n",
      "15         50      0.500       0.506667      0.065997\n",
      "16         50      1.000       0.513333      0.083799\n",
      "17         50      1.500       0.513333      0.083799\n",
      "18         75      0.001       0.423333      0.202868\n",
      "19         75      0.010       0.553333      0.067987\n",
      "20         75      0.100       0.516667      0.089938\n",
      "21         75      0.500       0.513333      0.067987\n",
      "22         75      1.000       0.513333      0.083799\n",
      "23         75      1.500       0.516667      0.092856\n",
      "24        100      0.001       0.446667      0.169967\n",
      "25        100      0.010       0.556667      0.108730\n",
      "26        100      0.100       0.510000      0.086410\n",
      "27        100      0.500       0.513333      0.083799\n",
      "28        100      1.000       0.513333      0.099778\n",
      "29        100      1.500       0.520000      0.101980\n",
      "..        ...        ...            ...           ...\n",
      "48        300      0.001       0.553333      0.052493\n",
      "49        300      0.010       0.530000      0.114310\n",
      "50        300      0.100       0.510000      0.058878\n",
      "51        300      0.500       0.520000      0.101980\n",
      "52        300      1.000       0.523333      0.095685\n",
      "53        300      1.500       0.523333      0.095685\n",
      "54        400      0.001       0.566667      0.108730\n",
      "55        400      0.010       0.526667      0.118134\n",
      "56        400      0.100       0.516667      0.077172\n",
      "57        400      0.500       0.523333      0.095685\n",
      "58        400      1.000       0.523333      0.095685\n",
      "59        400      1.500       0.523333      0.095685\n",
      "60        500      0.001       0.550000      0.099331\n",
      "61        500      0.010       0.516667      0.089938\n",
      "62        500      0.100       0.513333      0.083799\n",
      "63        500      0.500       0.523333      0.095685\n",
      "64        500      1.000       0.523333      0.095685\n",
      "65        500      1.500       0.523333      0.095685\n",
      "66        750      0.001       0.560000      0.071181\n",
      "67        750      0.010       0.520000      0.099331\n",
      "68        750      0.100       0.513333      0.083799\n",
      "69        750      0.500       0.523333      0.095685\n",
      "70        750      1.000       0.523333      0.095685\n",
      "71        750      1.500       0.523333      0.095685\n",
      "72       1000      0.001       0.556667      0.077172\n",
      "73       1000      0.010       0.513333      0.095685\n",
      "74       1000      0.100       0.513333      0.099778\n",
      "75       1000      0.500       0.523333      0.095685\n",
      "76       1000      1.000       0.523333      0.095685\n",
      "77       1000      1.500       0.523333      0.095685\n",
      "\n",
      "[78 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Using the training Grid and k-fold cross validation to train models\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 3\n",
    "\n",
    "#iterate over traning grid\n",
    "for index, row in trainGrid.iterrows():\n",
    "\n",
    "    #initialize accuraices (for each fold) to empty list\n",
    "    accuracies = list()\n",
    "\n",
    "    #Initial value for the Gradient Descent Parameter\n",
    "    step_size = row['step_size'] #Also called learning rate\n",
    "    num_iterations = int(row['num_iters']) # iterations\n",
    "\n",
    "    k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=0)\n",
    "\n",
    "    k = 1 #for printing\n",
    "    \n",
    "    #for each row of training grid (which specifies paramaters for training), perform k-fold cv to train\n",
    "    for train_indices, test_indices in k_fold.split(X):\n",
    "\n",
    "        # Start with an initialize parameters randomly, for each fold\n",
    "        #random weights\n",
    "        W = 0.01 * np.random.randn(D,K)\n",
    "        #zero bias terms\n",
    "        b = np.zeros((1,K))\n",
    "        \n",
    "        #training and testing for each iteration of k-fold cv\n",
    "        X_train_k = X[train_indices,]\n",
    "        X_test_k  = X[test_indices,]\n",
    "\n",
    "        y_train_k = y[train_indices]\n",
    "        y_test_k  = y[test_indices]\n",
    "\n",
    "        #For simplicity we will take the batch size to be the same as number of examples\n",
    "        num_examples = X_train_k.shape[0]\n",
    "\n",
    "        # gradient descent loop\n",
    "        for i in range(num_iterations): \n",
    "\n",
    "            # evaluate class scores, [N x K]\n",
    "            # X = 300 obs * 2 features || W = 2 (features) * 3 (output classes) || b = bias weights\n",
    "            scores = np.dot(X_train_k, W) + b \n",
    "\n",
    "            # compute the class probabilities\n",
    "            exp_scores = np.exp(scores)\n",
    "\n",
    "\n",
    "            probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "            #Compute loss using snippet of LogLoss fucntion created in Q3\n",
    "            #convert the y label to a N * K array \n",
    "            actual = np.zeros(probs.shape)\n",
    "            rows = actual.shape[0]\n",
    "\n",
    "            actual[np.arange(rows), y_train_k.astype(int)] = 1\n",
    "\n",
    "            #compute data loss\n",
    "            data_loss = -1.0 / rows * (np.sum(actual * np.log(probs)))\n",
    "\n",
    "            # L2 regularization term\n",
    "            L2 = np.sum(W ** 2) #not multiplied by 0.5\n",
    "\n",
    "            # the total loss\n",
    "            loss = data_loss + reg * L2\n",
    "\n",
    "            # compute the gradient on scores\n",
    "            dscores = probs\n",
    "            dscores[range(num_examples),y_train_k] -= 1\n",
    "            dscores /= num_examples\n",
    "\n",
    "            # backpropate the gradient to the parameters (W,b)\n",
    "            dW = np.dot(X_train_k.T, dscores)\n",
    "            db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "            dW += reg*W # regularization gradient\n",
    "\n",
    "            # perform a parameter update\n",
    "            W += -step_size * dW\n",
    "            b += -step_size * db\n",
    "\n",
    "\n",
    "        #compute scores for each fold\n",
    "        scores = np.dot(X_test_k, W) + b\n",
    "        predicted_class = np.argmax(scores, axis=1)\n",
    "        #accuracy for fold\n",
    "        fold_accuracy = np.mean(predicted_class == y_test_k)\n",
    "\n",
    "        #print 'validation accuracy for fold %d: %.2f' % (k, fold_accuracy)\n",
    "        accuracies.append(fold_accuracy)\n",
    "\n",
    "        k += 1 #only for printing purposes\n",
    "    \n",
    "    #set mean accuracy and accuracy std in training grid\n",
    "    trainGrid.set_value(index,'accuracy_mean', np.mean(accuracies))\n",
    "    trainGrid.set_value(index,'accuracy_std', np.std(accuracies) * 2)\n",
    "\n",
    "#Printing the accuracies and std deviations now\n",
    "print (trainGrid) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    num_iters  step_size  accuracy_mean  accuracy_std\n",
      "54        400      0.001       0.566667      0.108730\n",
      "2          10      0.100       0.563333      0.067987\n",
      "66        750      0.001       0.560000      0.071181\n",
      "25        100      0.010       0.556667      0.108730\n",
      "72       1000      0.001       0.556667      0.077172\n",
      "48        300      0.001       0.553333      0.052493\n",
      "19         75      0.010       0.553333      0.067987\n",
      "60        500      0.001       0.550000      0.099331\n",
      "31        150      0.010       0.546667      0.120370\n",
      "43        250      0.010       0.536667      0.118134\n",
      "13         50      0.010       0.536667      0.065997\n",
      "37        200      0.010       0.533333      0.094281\n",
      "8          25      0.100       0.533333      0.123648\n",
      "36        200      0.001       0.530000      0.086410\n",
      "49        300      0.010       0.530000      0.114310\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "# Writing results to csv for plotting\n",
    "# trainGrid.to_csv('assignment1_4_5.csv')\n",
    "\n",
    "topResults = trainGrid.sort_values('accuracy_mean', ascending=0)\n",
    "\n",
    "print (topResults.head(15))\n",
    "\n",
    "print (topResults['step_size'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 6 : What is the sensitivity of the model’s performance to different regularization parameter values. Find the best \n",
    "#              regularization parameter using an exhaustive search procedure. Describe your choice via suitable plots. What is \n",
    "#              the performance difference between using regularization and no regularization?\n",
    "\n",
    "#Using the best fit found in the last part, we'll try to find the best regularization parameter:\n",
    "\n",
    "trainGrid_reg = expand_grid({ 'reg': np.arange(0, 1, 0.01),\n",
    "                              'num_iters':[200],\n",
    "                              'step_size': [1e-0],\n",
    "                              'accuracy_mean': [0.0], #create a column to store accuracies in\n",
    "                              'accuracy_std' : [0.0] #create a column to store accuract std's in\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     reg  num_iters  step_size  accuracy_mean  accuracy_std\n",
      "0   0.00        200        1.0         0.5400      0.100995\n",
      "1   0.01        200        1.0         0.5275      0.087607\n",
      "2   0.02        200        1.0         0.5300      0.081240\n",
      "3   0.03        200        1.0         0.5300      0.081240\n",
      "4   0.04        200        1.0         0.5250      0.094340\n",
      "5   0.05        200        1.0         0.5300      0.103923\n",
      "6   0.06        200        1.0         0.5275      0.107121\n",
      "7   0.07        200        1.0         0.5300      0.100995\n",
      "8   0.08        200        1.0         0.5300      0.100995\n",
      "9   0.09        200        1.0         0.5325      0.095263\n",
      "10  0.10        200        1.0         0.5325      0.095263\n",
      "11  0.11        200        1.0         0.5375      0.105238\n",
      "12  0.12        200        1.0         0.5425      0.115217\n",
      "13  0.13        200        1.0         0.5375      0.105238\n",
      "14  0.14        200        1.0         0.5375      0.105238\n",
      "15  0.15        200        1.0         0.5450      0.110000\n",
      "16  0.16        200        1.0         0.5500      0.120000\n",
      "17  0.17        200        1.0         0.5500      0.120000\n",
      "18  0.18        200        1.0         0.5500      0.120000\n",
      "19  0.19        200        1.0         0.5575      0.105238\n",
      "20  0.20        200        1.0         0.5550      0.090000\n",
      "21  0.21        200        1.0         0.5575      0.085294\n",
      "22  0.22        200        1.0         0.5675      0.085294\n",
      "23  0.23        200        1.0         0.5675      0.065383\n",
      "24  0.24        200        1.0         0.5675      0.065383\n",
      "25  0.25        200        1.0         0.5725      0.075333\n",
      "26  0.26        200        1.0         0.5725      0.075333\n",
      "27  0.27        200        1.0         0.5825      0.095263\n",
      "28  0.28        200        1.0         0.5850      0.091104\n",
      "29  0.29        200        1.0         0.5850      0.091104\n",
      "..   ...        ...        ...            ...           ...\n",
      "70  0.70        200        1.0         0.6125      0.084113\n",
      "71  0.71        200        1.0         0.6125      0.084113\n",
      "72  0.72        200        1.0         0.6125      0.084113\n",
      "73  0.73        200        1.0         0.6100      0.092736\n",
      "74  0.74        200        1.0         0.6075      0.090967\n",
      "75  0.75        200        1.0         0.6075      0.090967\n",
      "76  0.76        200        1.0         0.6075      0.090967\n",
      "77  0.77        200        1.0         0.6050      0.090000\n",
      "78  0.78        200        1.0         0.6050      0.090000\n",
      "79  0.79        200        1.0         0.6075      0.098362\n",
      "80  0.80        200        1.0         0.6025      0.089861\n",
      "81  0.81        200        1.0         0.6025      0.089861\n",
      "82  0.82        200        1.0         0.6025      0.089861\n",
      "83  0.83        200        1.0         0.6025      0.089861\n",
      "84  0.84        200        1.0         0.5975      0.098362\n",
      "85  0.85        200        1.0         0.5925      0.089861\n",
      "86  0.86        200        1.0         0.5925      0.089861\n",
      "87  0.87        200        1.0         0.5925      0.089861\n",
      "88  0.88        200        1.0         0.5900      0.097980\n",
      "89  0.89        200        1.0         0.5900      0.097980\n",
      "90  0.90        200        1.0         0.5900      0.097980\n",
      "91  0.91        200        1.0         0.5900      0.097980\n",
      "92  0.92        200        1.0         0.5875      0.098362\n",
      "93  0.93        200        1.0         0.5875      0.098362\n",
      "94  0.94        200        1.0         0.5775      0.081701\n",
      "95  0.95        200        1.0         0.5725      0.073993\n",
      "96  0.96        200        1.0         0.5725      0.073993\n",
      "97  0.97        200        1.0         0.5725      0.073993\n",
      "98  0.98        200        1.0         0.5775      0.081701\n",
      "99  0.99        200        1.0         0.5775      0.081701\n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Using the training Grid and k-fold cross validation to train models\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 3\n",
    "\n",
    "#iterate over traning grid\n",
    "for index, row in trainGrid_reg.iterrows():\n",
    "    \n",
    "    #set reg other than global\n",
    "    reg = row['reg']\n",
    "    \n",
    "    #initialize accuraices (for each fold) to empty list\n",
    "    accuracies = list()\n",
    "\n",
    "    #Initial value for the Gradient Descent Parameter\n",
    "    step_size = row['step_size'] #Also called learning rate\n",
    "    num_iterations = int(row['num_iters']) #200 iterations\n",
    "\n",
    "    k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=0)\n",
    "\n",
    "    k = 1 #for printing\n",
    "    \n",
    "    #for each row of training grid (which specifies paramaters for training), perform k-fold cv to train\n",
    "    for train_indices, test_indices in k_fold.split(X):\n",
    "\n",
    "        # Start with an initialize parameters randomly, for each fold\n",
    "        #random weights\n",
    "        W = 0.01 * np.random.randn(D,K)\n",
    "        #zero bias terms\n",
    "        b = np.zeros((1,K))\n",
    "        \n",
    "        #training and testing for each iteration of k-fold cv\n",
    "        X_train_k = X[train_indices,]\n",
    "        X_test_k  = X[test_indices,]\n",
    "\n",
    "        y_train_k = y[train_indices]\n",
    "        y_test_k  = y[test_indices]\n",
    "\n",
    "        #For simplicity we will take the batch size to be the same as number of examples\n",
    "        num_examples = X_train_k.shape[0]\n",
    "\n",
    "        # gradient descent loop\n",
    "        for i in range(num_iterations): \n",
    "\n",
    "            # evaluate class scores, [N x K]\n",
    "            # X = 300 obs * 2 features || W = 2 (features) * 3 (output classes) || b = bias weights\n",
    "            scores = np.dot(X_train_k, W) + b \n",
    "\n",
    "            # compute the class probabilities\n",
    "            exp_scores = np.exp(scores)\n",
    "\n",
    "\n",
    "            probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "            #Compute loss using snippet of LogLoss fucntion created in Q3\n",
    "            #convert the y label to a N * K array \n",
    "            actual = np.zeros(probs.shape)\n",
    "            rows = actual.shape[0]\n",
    "\n",
    "            actual[np.arange(rows), y_train_k.astype(int)] = 1\n",
    "\n",
    "            #compute data loss\n",
    "            data_loss = -1.0 / rows * (np.sum(actual * np.log(probs)))\n",
    "\n",
    "            # L2 regularization term\n",
    "            L2 = np.sum(W ** 2) #not multiplied by 0.5\n",
    "\n",
    "            # the total loss\n",
    "            loss = data_loss + reg * L2\n",
    "\n",
    "            # compute the gradient on scores\n",
    "            dscores = probs\n",
    "            dscores[range(num_examples),y_train_k] -= 1\n",
    "            dscores /= num_examples\n",
    "\n",
    "            # backpropate the gradient to the parameters (W,b)\n",
    "            dW = np.dot(X_train_k.T, dscores)\n",
    "            db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "            dW += reg*W # regularization gradient\n",
    "\n",
    "            # perform a parameter update\n",
    "            W += -step_size * dW\n",
    "            \n",
    "#use the training Grid and k-fold cross validation to train models\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 3\n",
    "\n",
    "#iterate over traning grid\n",
    "for index, row in trainGrid_reg.iterrows():\n",
    "    \n",
    "    #set reg other than global\n",
    "    reg = row['reg']\n",
    "    \n",
    "    #initialize accuraices (for each fold) to empty list\n",
    "    accuracies = list()\n",
    "\n",
    "    #Initial value for the Gradient Descent Parameter\n",
    "    step_size = row['step_size'] #Also called learning rate\n",
    "    num_iterations = int(row['num_iters']) #200 iterations\n",
    "\n",
    "    k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=0)\n",
    "\n",
    "    k = 1 #for printing\n",
    "    \n",
    "    #for each row of training grid (which specifies paramaters for training), perform k-fold cv to train\n",
    "    for train_indices, test_indices in k_fold.split(X):\n",
    "\n",
    "        # Start with an initialize parameters randomly, for each fold\n",
    "        #random weights\n",
    "        W = 0.01 * np.random.randn(D,K)\n",
    "        #zero bias terms\n",
    "        b = np.zeros((1,K))\n",
    "        \n",
    "        #training and testing for each iteration of k-fold cv\n",
    "        X_train_k = X[train_indices,]\n",
    "        X_test_k  = X[test_indices,]\n",
    "\n",
    "        y_train_k = y[train_indices]\n",
    "        y_test_k  = y[test_indices]\n",
    "\n",
    "        #For simplicity we will take the batch size to be the same as number of examples\n",
    "        num_examples = X_train_k.shape[0]\n",
    "\n",
    "        # gradient descent loop\n",
    "        for i in range(num_iterations): \n",
    "\n",
    "            # evaluate class scores, [N x K]\n",
    "            # X = 300 obs * 2 features || W = 2 (features) * 3 (output classes) || b = bias weights\n",
    "            scores = np.dot(X_train_k, W) + b \n",
    "\n",
    "            # compute the class probabilities\n",
    "            exp_scores = np.exp(scores)\n",
    "\n",
    "\n",
    "            probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "            #Compute loss using snippet of LogLoss fucntion created in Q3\n",
    "            #convert the y label to a N * K array \n",
    "            actual = np.zeros(probs.shape)\n",
    "            rows = actual.shape[0]\n",
    "\n",
    "            actual[np.arange(rows), y_train_k.astype(int)] = 1\n",
    "\n",
    "            #compute data loss\n",
    "            data_loss = -1.0 / rows * (np.sum(actual * np.log(probs)))\n",
    "\n",
    "            # L2 regularization term\n",
    "            L2 = np.sum(W ** 2) #not multiplied by 0.5\n",
    "\n",
    "            # the total loss\n",
    "            loss = data_loss + reg * L2\n",
    "\n",
    "            # compute the gradient on scores\n",
    "            dscores = probs\n",
    "            dscores[range(num_examples),y_train_k] -= 1\n",
    "            dscores /= num_examples\n",
    "\n",
    "            # backpropate the gradient to the parameters (W,b)\n",
    "            dW = np.dot(X_train_k.T, dscores)\n",
    "            db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "            dW += reg*W # regularization gradient\n",
    "\n",
    "            # perform a parameter update\n",
    "            W += -step_size * dW\n",
    "            b += -step_size * db\n",
    "\n",
    "\n",
    "        #compute scores for each fold\n",
    "        scores = np.dot(X_test_k, W) + b\n",
    "        predicted_class = np.argmax(scores, axis=1)\n",
    "        #accuracy for fold\n",
    "        fold_accuracy = np.mean(predicted_class == y_test_k)\n",
    "\n",
    "        #print 'validation accuracy for fold %d: %.2f' % (k, fold_accuracy)\n",
    "        accuracies.append(fold_accuracy)\n",
    "\n",
    "        k += 1 #only for printing purposes\n",
    "    \n",
    "    #set mean accuracy and accuracy std in training grid\n",
    "    trainGrid_reg.set_value(index,'accuracy_mean', np.mean(accuracies))\n",
    "    trainGrid_reg.set_value(index,'accuracy_std', np.std(accuracies) * 2)\n",
    "    b += -step_size * db\n",
    "\n",
    "\n",
    "    #compute scores for each fold\n",
    "    scores = np.dot(X_test_k, W) + b\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    #accuracy for fold\n",
    "    fold_accuracy = np.mean(predicted_class == y_test_k)\n",
    "\n",
    "    #print 'validation accuracy for fold %d: %.2f' % (k, fold_accuracy)\n",
    "    accuracies.append(fold_accuracy)\n",
    "\n",
    "    k += 1 #only for printing purposes\n",
    "    \n",
    "    #set mean accuracy and accuracy std in training grid\n",
    "    trainGrid_reg.set_value(index,'accuracy_mean', np.mean(accuracies))\n",
    "    trainGrid_reg.set_value(index,'accuracy_std', np.std(accuracies) * 2)\n",
    "    \n",
    "print (trainGrid_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     reg  num_iters  step_size  accuracy_mean  accuracy_std\n",
      "52  0.52        200        1.0         0.6325      0.065383\n",
      "53  0.53        200        1.0         0.6325      0.065383\n",
      "54  0.54        200        1.0         0.6325      0.065383\n",
      "51  0.51        200        1.0         0.6300      0.066332\n",
      "55  0.55        200        1.0         0.6300      0.066332\n",
      "50  0.50        200        1.0         0.6250      0.075498\n",
      "48  0.48        200        1.0         0.6250      0.091104\n",
      "56  0.56        200        1.0         0.6250      0.057446\n",
      "49  0.49        200        1.0         0.6225      0.077942\n",
      "60  0.60        200        1.0         0.6225      0.049749\n",
      "61  0.61        200        1.0         0.6225      0.049749\n",
      "58  0.58        200        1.0         0.6200      0.048990\n",
      "57  0.57        200        1.0         0.6200      0.048990\n",
      "62  0.62        200        1.0         0.6200      0.058310\n",
      "63  0.63        200        1.0         0.6200      0.058310\n"
     ]
    }
   ],
   "source": [
    "#write results to csv for plotting\n",
    "#trainGrid_reg.to_csv('assignment1_4_6.csv')\n",
    "\n",
    "topResults_reg = trainGrid_reg.sort_values('accuracy_mean', ascending=0)\n",
    "\n",
    "print (topResults_reg.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test_size  accuracy\n",
      "0       0.05       0.0\n",
      "1       0.10       0.0\n",
      "2       0.15       0.0\n",
      "3       0.20       0.0\n",
      "4       0.25       0.0\n",
      "5       0.30       0.0\n",
      "6       0.35       0.0\n",
      "7       0.40       0.0\n",
      "8       0.45       0.0\n",
      "9       0.50       0.0\n"
     ]
    }
   ],
   "source": [
    "# Question 7 : What is the sensitivity of the model’s performance with respect to a different test train split (50%:50%).\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#trainX, testX, trainY, testY = train_test_split(X,y, test_size = test_size, stratify = y)\n",
    "\n",
    "trainGrid_diffSplits = expand_grid({ 'test_size' : np.arange(0.05, 0.55, 0.05),\n",
    "                                     'accuracy': [0.0]\n",
    "                          #'num_iters':[10,25,50,75,100,150, 200, 250, 300, 400,500,750,1000],\n",
    "                          #'step_size': [1e-3,1e-2,1e-1,5e-1,1,1.5],\n",
    "                          #'accuracy_mean': [0.0], #create a column to store accuracies in\n",
    "                          #'accuracy_std' : [0.0] #create a column to store accuract std's in\n",
    "                        })\n",
    "print (trainGrid_diffSplits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy with test split of 0.05: 0.60\n",
      "test accuracy with test split of 0.10: 0.53\n",
      "test accuracy with test split of 0.15: 0.54\n",
      "test accuracy with test split of 0.20: 0.45\n",
      "test accuracy with test split of 0.25: 0.51\n",
      "test accuracy with test split of 0.30: 0.50\n",
      "test accuracy with test split of 0.35: 0.47\n",
      "test accuracy with test split of 0.40: 0.49\n",
      "test accuracy with test split of 0.45: 0.50\n",
      "test accuracy with test split of 0.50: 0.55\n"
     ]
    }
   ],
   "source": [
    "#Initial value for the Gradient Descent Parameter\n",
    "step_size = 1e-0 #Also called learning rate\n",
    "\n",
    "num_iterations = 200 #200 iterations\n",
    "\n",
    "#Initialize hyper-parameter value (not optimized)\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "#iterate over traning grid\n",
    "for index, row in trainGrid_diffSplits.iterrows():\n",
    "    #random weights\n",
    "    W = 0.01 * np.random.randn(D,K)\n",
    "    #zero bias terms\n",
    "    b = np.zeros((1,K))\n",
    "\n",
    "    trainX, testX, trainY, testY = train_test_split(X,y, test_size = row['test_size'], stratify = y)\n",
    "    \n",
    "    num_examples = trainX.shape[0]\n",
    "    \n",
    "    # gradient descent loop\n",
    "    for i in range(num_iterations): \n",
    "\n",
    "        # evaluate class scores, [N x K]\n",
    "        # X = 300 obs * 2 features || W = 2 (features) * 3 (output classes) || b = bias weights\n",
    "        scores = np.dot(trainX, W) + b \n",
    "\n",
    "        # compute the class probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "        #Compute loss using snippet of LogLoss fucntion created in Q3\n",
    "        #convert the y label to a N * K array \n",
    "        actual = np.zeros(probs.shape)\n",
    "        rows = actual.shape[0]\n",
    "\n",
    "        actual[np.arange(rows), trainY.astype(int)] = 1\n",
    "\n",
    "        #compute data loss\n",
    "        data_loss = -1.0 / rows * (np.sum(actual * np.log(probs)))\n",
    "\n",
    "        # L2 regularization term\n",
    "        L2 = np.sum(W ** 2) #not multiplied by 0.5\n",
    "\n",
    "        # the total loss\n",
    "        loss = data_loss + reg * L2\n",
    "\n",
    "        #if i % 10 == 0:\n",
    "            #print \"iteration %d: loss %f\" % (i, loss)\n",
    "            #print \"iteration %d: loss %f\" % (i, loss_old)\n",
    "\n",
    "        # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),trainY] -= 1\n",
    "        dscores /= num_examples\n",
    "\n",
    "        # backpropate the gradient to the parameters (W,b)\n",
    "        dW = np.dot(trainX.T, dscores)\n",
    "        db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "        dW += reg*W # regularization gradient\n",
    "\n",
    "        # perform a parameter update\n",
    "        W += -step_size * dW\n",
    "        b += -step_size * db\n",
    "\n",
    "    scores = np.dot(testX, W) + b\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    trainGrid_diffSplits.set_value(index,'accuracy', np.mean(predicted_class == testY))\n",
    "    print ('test accuracy with test split of %.2f: %.2f' % (row['test_size'], np.mean(predicted_class == testY)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now, use the parameters of the best model found in 4.6\n",
    "\n",
    "#reg  num_iters  step_size  accuracy_mean  accuracy_std\n",
    "#0.52        200        1.0         0.6325      0.065383\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy with test split of 0.05: 0.73\n",
      "test accuracy with test split of 0.10: 0.60\n",
      "test accuracy with test split of 0.15: 0.65\n",
      "test accuracy with test split of 0.20: 0.55\n",
      "test accuracy with test split of 0.25: 0.43\n",
      "test accuracy with test split of 0.30: 0.47\n",
      "test accuracy with test split of 0.35: 0.46\n",
      "test accuracy with test split of 0.40: 0.50\n",
      "test accuracy with test split of 0.45: 0.51\n",
      "test accuracy with test split of 0.50: 0.55\n"
     ]
    }
   ],
   "source": [
    "#Initial value for the Gradient Descent Parameter\n",
    "step_size = 1.0 #learning rate\n",
    "\n",
    "num_iterations = 200 # iterations\n",
    "\n",
    "#Initialize hyper-parameter value (optimized in 4.6)\n",
    "reg = 0.52 # regularization strength\n",
    "\n",
    "#iterate over traning grid\n",
    "for index, row in trainGrid_diffSplits.iterrows():\n",
    "    #random weights\n",
    "    W = 0.01 * np.random.randn(D,K)\n",
    "    #zero bias terms\n",
    "    b = np.zeros((1,K))\n",
    "\n",
    "    trainX, testX, trainY, testY = train_test_split(X,y, test_size = row['test_size'], stratify = y)\n",
    "    \n",
    "    num_examples = trainX.shape[0]\n",
    "    \n",
    "    # gradient descent loop\n",
    "    for i in range(num_iterations): \n",
    "\n",
    "        # evaluate class scores, [N x K]\n",
    "        # X = 300 obs * 2 features || W = 2 (features) * 3 (output classes) || b = bias weights\n",
    "        scores = np.dot(trainX, W) + b \n",
    "\n",
    "        # compute the class probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "        #Compute loss using snippet of LogLoss fucntion created in Q3\n",
    "        #convert the y label to a N * K array \n",
    "        actual = np.zeros(probs.shape)\n",
    "        rows = actual.shape[0]\n",
    "\n",
    "        actual[np.arange(rows), trainY.astype(int)] = 1\n",
    "\n",
    "        #compute data loss\n",
    "        data_loss = -1.0 / rows * (np.sum(actual * np.log(probs)))\n",
    "\n",
    "        # L2 regularization term\n",
    "        L2 = np.sum(W ** 2) #not multiplied by 0.5\n",
    "\n",
    "        # the total loss\n",
    "        loss = data_loss + reg * L2\n",
    "\n",
    "        #if i % 10 == 0:\n",
    "            #print \"iteration %d: loss %f\" % (i, loss)\n",
    "            #print \"iteration %d: loss %f\" % (i, loss_old)\n",
    "\n",
    "        # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),trainY] -= 1\n",
    "        dscores /= num_examples\n",
    "\n",
    "        # backpropate the gradient to the parameters (W,b)\n",
    "        dW = np.dot(trainX.T, dscores)\n",
    "        db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "        dW += reg*W # regularization gradient\n",
    "\n",
    "        # perform a parameter update\n",
    "        W += -step_size * dW\n",
    "        b += -step_size * db\n",
    "\n",
    "    scores = np.dot(testX, W) + b\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    trainGrid_diffSplits.set_value(index,'accuracy', np.mean(predicted_class == testY))\n",
    "    print ('test accuracy with test split of %.2f: %.2f' % (row['test_size'], np.mean(predicted_class == testY)))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
