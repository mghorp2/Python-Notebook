{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "np.random.seed(0) #For repeatability of the experiment\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) train samples\n",
      "(10000, 784) test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape, 'train samples')\n",
    "print(x_test.shape, 'test samples')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) train lables\n",
      "(10000,) test lables\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, 'train lables')\n",
    "print(y_test.shape, 'test lables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feedforward neural net model\n",
    "np.random.seed(0)\n",
    "D = x_train.shape[1] #Number of features\n",
    "K = max(y_train)+1 #Number of classes assuming class index starts from 0\n",
    "# Start with an initialize parameters randomly\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.05 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W_cv=W\n",
    "b_cv=b\n",
    "\n",
    "W2 = 0.05 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "W2_cv=W2\n",
    "b2_cv=b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient descent loop for relu\n",
    "def grad_descent_relu(xtrain, xtest, ytrain, ytest,W,W2,b,b2,reg,step_size,num_examples,iterations):\n",
    "    \n",
    "    for i in range(iterations):\n",
    "\n",
    "          # evaluate class scores, [N x K]\n",
    "        hidden_layer = np.maximum(0, np.dot(xtrain, W) + b) # note, ReLU activation\n",
    "        scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "        # compute the class probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "        # compute the loss: average cross-entropy loss and regularization\n",
    "        corect_logprobs = -np.log(probs[range(num_examples),ytrain])\n",
    "        data_loss = np.sum(corect_logprobs)/num_examples\n",
    "        reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "        loss = data_loss + reg_loss\n",
    "#         if i % 500 == 0:\n",
    "#              print(\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "        # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),ytrain] -= 1\n",
    "        dscores /= num_examples\n",
    "\n",
    "          # backpropate the gradient to the parameters\n",
    "        # first backprop into parameters W2 and b2\n",
    "        dW2 = np.dot(hidden_layer.T, dscores)\n",
    "        db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "        # next backprop into hidden layer\n",
    "        dhidden = np.dot(dscores, W2.T)\n",
    "        # backprop the ReLU non-linearity\n",
    "        dhidden[hidden_layer <= 0] = 0\n",
    "        # finally into W,b\n",
    "        dW = np.dot(xtrain.T, dhidden)\n",
    "        db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "        # add regularization gradient contribution\n",
    "        dW2 += reg * W2\n",
    "        dW += reg * W\n",
    "\n",
    "        # perform a parameter update\n",
    "        W += -step_size * dW\n",
    "        b += -step_size * db\n",
    "        W2 += -step_size * dW2\n",
    "        b2 += -step_size * db2\n",
    "\n",
    "\n",
    "    hidden_layer = np.maximum(0, np.dot(xtest, W) + b)\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    Accuracy=np.mean(predicted_class == ytest)\n",
    "#     print('Accuracy: %.2f' % Accuracy)\n",
    "    \n",
    "           \n",
    "    return W, W2,b,b2,Accuracy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Initial values from hyperparameter\n",
    "reg = 1e-4 # regularization strength\n",
    "#For simplicity we will take the batch size to be the same as number of examples\n",
    "num_examples = x_train.shape[0]\n",
    "#Initial value for the Gradient Descent Parameter\n",
    "step_size = 1e-1 #Also called learning rate\n",
    "iterations=1000\n",
    "W,W2,b,b2,Test_Accuracy = grad_descent_relu(x_train, x_test, y_train, y_test,W,W2,b,b2,reg,step_size,num_examples,iterations)\n",
    "print('Test Accuracy: %.2f' % Test_Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 2.301610\n",
      "iteration 500: loss 0.341049\n",
      "Accuracy: 0.92\n",
      "iteration 0: loss 0.275764\n",
      "iteration 500: loss 0.229687\n",
      "Accuracy: 0.94\n",
      "iteration 0: loss 0.213695\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-90c91ba4b649>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mw2_cv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mnum_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad_descent_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_cv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw2_cv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mB_cv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mB2_cv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mvalidation_accuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#print('Validation Accuracy for fold %d: %.2f' % (k, accuracy))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-be795e0eebf8>\u001b[0m in \u001b[0;36mgrad_descent_relu\u001b[1;34m(xtrain, xtest, ytrain, ytest, W, W2, b, b2, reg, step_size, num_examples, iterations)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mdhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhidden_layer\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# finally into W,b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "D = x_train.shape[1] #Number of features\n",
    "h = 100\n",
    "K = max(y_train)+1\n",
    "reg = 1e-4 # regularization strength\n",
    "#Initial value for the Gradient Descent Parameter\n",
    "step_size = 1e-1\n",
    "iterations=1000\n",
    "kfold = KFold(3)\n",
    "validation_accuracy=[]\n",
    "factor=[0.001,0.01,0.05,0.5,1]\n",
    "np.random.seed(0)\n",
    "for item in factor:\n",
    "    W = item*np.random.randn(D,h)\n",
    "    b = np.zeros((1,h))\n",
    "    W2= item * np.random.randn(h,K)\n",
    "    b2=np.zeros((1,K))\n",
    "    for k,(train,test) in enumerate(kfold.split(x_train,y_train)):\n",
    "        w_cv=W\n",
    "        B_cv=b\n",
    "        B2_cv=b2\n",
    "        w2_cv=W2\n",
    "        num_examples=x_train[train].shape[0]\n",
    "        w,w2,b,b2,accuracy=grad_descent_relu(x_train[train], x_train[test], y_train[train], y_train[test],w_cv,w2_cv,B_cv,B2_cv,reg,step_size,num_examples,iterations)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        #print('Validation Accuracy for fold %d: %.2f' % (k, accuracy))\n",
    "        #print('done processing for k: %d' % k)\n",
    "    \n",
    "    #Avg_validation_accuracy.append(np.mean(validation_accuracy))\n",
    "    print(item)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.mean(validation_accuracy), np.std(validation_accuracy) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
