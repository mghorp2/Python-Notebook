{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "np.random.seed(0) #For repeatability of the experiment\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) train samples\n",
      "(10000, 784) test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape, 'train samples')\n",
    "print(x_test.shape, 'test samples')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) train lables\n",
      "(10000,) test lables\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, 'train lables')\n",
    "print(y_test.shape, 'test lables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feedforward neural net model\n",
    "np.random.seed(0)\n",
    "D = x_train.shape[1] #Number of features\n",
    "K = max(y_train)+1 #Number of classes assuming class index starts from 0\n",
    "# Start with an initialize parameters randomly\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W_cv=W\n",
    "b_cv=b\n",
    "\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "W2_cv=W2\n",
    "b2_cv=b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient descent loop for relu\n",
    "def grad_descent_relu(xtrain, xtest, ytrain, ytest,W,W2,b,b2,reg,step_size,num_examples,iterations):\n",
    "    \n",
    "    for i in range(iterations):\n",
    "\n",
    "          # evaluate class scores, [N x K]\n",
    "        hidden_layer = np.maximum(0, np.dot(xtrain, W) + b) # note, ReLU activation\n",
    "        scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "        # compute the class probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "        # compute the loss: average cross-entropy loss and regularization\n",
    "        corect_logprobs = -np.log(probs[range(num_examples),ytrain])\n",
    "        data_loss = np.sum(corect_logprobs)/num_examples\n",
    "        reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "        loss = data_loss + reg_loss\n",
    "        if i % 500 == 0:\n",
    "             print(\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "        # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),ytrain] -= 1\n",
    "        dscores /= num_examples\n",
    "\n",
    "          # backpropate the gradient to the parameters\n",
    "        # first backprop into parameters W2 and b2\n",
    "        dW2 = np.dot(hidden_layer.T, dscores)\n",
    "        db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "        # next backprop into hidden layer\n",
    "        dhidden = np.dot(dscores, W2.T)\n",
    "        # backprop the ReLU non-linearity\n",
    "        dhidden[hidden_layer <= 0] = 0\n",
    "        # finally into W,b\n",
    "        dW = np.dot(xtrain.T, dhidden)\n",
    "        db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "        # add regularization gradient contribution\n",
    "        dW2 += reg * W2\n",
    "        dW += reg * W\n",
    "\n",
    "        # perform a parameter update\n",
    "        W += -step_size * dW\n",
    "        b += -step_size * db\n",
    "        W2 += -step_size * dW2\n",
    "        b2 += -step_size * db2\n",
    "\n",
    "\n",
    "    hidden_layer = np.maximum(0, np.dot(xtest, W) + b)\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    Accuracy=np.mean(predicted_class == ytest)\n",
    "    print('Accuracy: %.2f' % Accuracy)\n",
    "    \n",
    "           \n",
    "    return W, W2,b,b2,Accuracy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 2.301597\n",
      "iteration 10: loss 2.291893\n",
      "iteration 20: loss 2.272737\n",
      "iteration 30: loss 2.231748\n",
      "iteration 40: loss 2.150246\n",
      "iteration 50: loss 2.010970\n",
      "iteration 60: loss 1.808705\n",
      "iteration 70: loss 1.564001\n",
      "iteration 80: loss 1.325558\n",
      "iteration 90: loss 1.131215\n",
      "iteration 100: loss 0.984713\n",
      "iteration 110: loss 0.875289\n",
      "iteration 120: loss 0.792381\n",
      "iteration 130: loss 0.728261\n",
      "iteration 140: loss 0.677582\n",
      "iteration 150: loss 0.636677\n",
      "iteration 160: loss 0.603021\n",
      "iteration 170: loss 0.574853\n",
      "iteration 180: loss 0.550931\n",
      "iteration 190: loss 0.530358\n",
      "iteration 200: loss 0.512481\n",
      "iteration 210: loss 0.496805\n",
      "iteration 220: loss 0.482955\n",
      "iteration 230: loss 0.470635\n",
      "iteration 240: loss 0.459611\n",
      "iteration 250: loss 0.449696\n",
      "iteration 260: loss 0.440733\n",
      "iteration 270: loss 0.432592\n",
      "iteration 280: loss 0.425163\n",
      "iteration 290: loss 0.418356\n",
      "iteration 300: loss 0.412092\n",
      "iteration 310: loss 0.406309\n",
      "iteration 320: loss 0.400949\n",
      "iteration 330: loss 0.395961\n",
      "iteration 340: loss 0.391306\n",
      "iteration 350: loss 0.386945\n",
      "iteration 360: loss 0.382849\n",
      "iteration 370: loss 0.378989\n",
      "iteration 380: loss 0.375341\n",
      "iteration 390: loss 0.371884\n",
      "iteration 400: loss 0.368600\n",
      "iteration 410: loss 0.365473\n",
      "iteration 420: loss 0.362493\n",
      "iteration 430: loss 0.359647\n",
      "iteration 440: loss 0.356923\n",
      "iteration 450: loss 0.354308\n",
      "iteration 460: loss 0.351795\n",
      "iteration 470: loss 0.349376\n",
      "iteration 480: loss 0.347047\n",
      "iteration 490: loss 0.344796\n",
      "iteration 500: loss 0.342620\n",
      "iteration 510: loss 0.340513\n",
      "iteration 520: loss 0.338470\n",
      "iteration 530: loss 0.336489\n",
      "iteration 540: loss 0.334563\n",
      "iteration 550: loss 0.332690\n",
      "iteration 560: loss 0.330865\n",
      "iteration 570: loss 0.329086\n",
      "iteration 580: loss 0.327349\n",
      "iteration 590: loss 0.325651\n",
      "iteration 600: loss 0.323990\n",
      "iteration 610: loss 0.322368\n",
      "iteration 620: loss 0.320779\n",
      "iteration 630: loss 0.319218\n",
      "iteration 640: loss 0.317684\n",
      "iteration 650: loss 0.316180\n",
      "iteration 660: loss 0.314704\n",
      "iteration 670: loss 0.313253\n",
      "iteration 680: loss 0.311830\n",
      "iteration 690: loss 0.310435\n",
      "iteration 700: loss 0.309062\n",
      "iteration 710: loss 0.307711\n",
      "iteration 720: loss 0.306379\n",
      "iteration 730: loss 0.305067\n",
      "iteration 740: loss 0.303775\n",
      "iteration 750: loss 0.302500\n",
      "iteration 760: loss 0.301243\n",
      "iteration 770: loss 0.300002\n",
      "iteration 780: loss 0.298777\n",
      "iteration 790: loss 0.297567\n",
      "iteration 800: loss 0.296371\n",
      "iteration 810: loss 0.295190\n",
      "iteration 820: loss 0.294024\n",
      "iteration 830: loss 0.292870\n",
      "iteration 840: loss 0.291728\n",
      "iteration 850: loss 0.290598\n",
      "iteration 860: loss 0.289482\n",
      "iteration 870: loss 0.288375\n",
      "iteration 880: loss 0.287280\n",
      "iteration 890: loss 0.286196\n",
      "iteration 900: loss 0.285122\n",
      "iteration 910: loss 0.284058\n",
      "iteration 920: loss 0.283002\n",
      "iteration 930: loss 0.281953\n",
      "iteration 940: loss 0.280915\n",
      "iteration 950: loss 0.279884\n",
      "iteration 960: loss 0.278864\n",
      "iteration 970: loss 0.277851\n",
      "iteration 980: loss 0.276845\n",
      "iteration 990: loss 0.275846\n",
      "Accuracy: 0.92\n",
      "Test Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Initial values from hyperparameter\n",
    "reg = 1e-4 # regularization strength\n",
    "#For simplicity we will take the batch size to be the same as number of examples\n",
    "num_examples = x_train.shape[0]\n",
    "#Initial value for the Gradient Descent Parameter\n",
    "step_size = 1e-1 #Also called learning rate\n",
    "iterations=1000\n",
    "W,W2,b,b2,Test_Accuracy = grad_descent_relu(x_train, x_test, y_train, y_test,W,W2,b,b2,reg,step_size,num_examples,iterations)\n",
    "print('Test Accuracy: %.2f' % Test_Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 2.301610\n"
     ]
    }
   ],
   "source": [
    "D = x_train.shape[1] #Number of features\n",
    "h = 100\n",
    "K = max(y_train)+1\n",
    "reg = 1e-4 # regularization strength\n",
    "#Initial value for the Gradient Descent Parameter\n",
    "step_size = 1e-1\n",
    "iterations=1000\n",
    "kfold = KFold(3)\n",
    "validation_accuracy=[]\n",
    "factor=[0.01,0.05,0.5,1]\n",
    "np.random.seed(0)\n",
    "for item in factor:\n",
    "    W = item*np.random.randn(D,h)\n",
    "    b = np.zeros((1,h))\n",
    "    W2= item * np.random.randn(h,K)\n",
    "    b2=np.zeros((1,K))\n",
    "    for k,(train,test) in enumerate(kfold.split(x_train,y_train)):\n",
    "        w_cv=W\n",
    "        B_cv=b\n",
    "        B2_cv=b2\n",
    "        w2_cv=W2\n",
    "        num_examples=x_train[train].shape[0]\n",
    "        w,w2,b,b2,accuracy=grad_descent_relu(x_train[train], x_train[test], y_train[train], y_train[test],w_cv,w2_cv,B_cv,B2_cv,reg,step_size,num_examples,iterations)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        #print('Validation Accuracy for fold %d: %.2f' % (k, accuracy))\n",
    "        #print('done processing for k: %d' % k)\n",
    "    \n",
    "    #Avg_validation_accuracy.append(np.mean(validation_accuracy))\n",
    "    print(item)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.mean(validation_accuracy), np.std(validation_accuracy) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
